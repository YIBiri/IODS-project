# Clustering and classification

_In this analysis, we apply clustering and classification methods to study a dataset. Based on classification from training data we are able to classify also training data._

## Introduction 
```{r echo = F}
library(MASS)
library(ggplot2)
library(GGally)
library(tidyverse)
library(corrplot)
```

### Data description
From the R package "MASS", the data we shall use for this analysis represents housing values in Boston from the 1970s. A quick overview of the variables is provided below, where the variables stand for the following ([Source](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)):

* crim: per capita crime rate by town.

*zn: proportion of residential land zoned for lots over 25,000 sq.ft.

*indus: proportion of non-retail business acres per town.

* chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

* nox: nitrogen oxides concentration (parts per 10 million).

* rm: average number of rooms per dwelling.

* age: proportion of owner-occupied units built prior to 1940.

* dis: weighted mean of distances to five Boston employment centres.

* rad: index of accessibility to radial highways.

* tax: full-value property-tax rate per \$10,000.

* ptratio: pupil-teacher ratio by town.

* black: 1000(Bkâˆ’0.63)^2 where Bk is the proportion of blacks by town.

* lstat: lower status of the population (percent).

* medv: median value of owner-occupied homes in \$1000s.

```{r echo = F}
data(Boston)
summary(Boston)
```
The variable _rad_ and the dummy variable _chas_ are the only integer variables, the remaining being numerical. 

### Graphical overview

```{r echo = F}
cor_matrix<-cor(Boston) %>% corrplot(method="circle", type = "upper", tl.pos = "d", tl.cex = 0.8)
```

### Scaling the data

Before we begin our analysis, we shall standardise the dataset. This means that we assume normal distribution and adjust the numeric values of variables so that mean = 0, all values indicating a distance from the mean in units of standard deviation. The data now looks as follows (cf. with table above):

```{r echo = F}
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
summary(boston_scaled)
```

Before continuing, we shall modify the variable _crim_ into a categorical factor variable based on its quantiles, seen above as its values for Min, 1st Qu., Median, 3rd Qu., and Max. The variable (renamed _crime_) now has the following factors and observations per factor:

```{r echo = F}
#create quantiles for crim
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

table(boston_scaled$crime)
```

We shall also divide the dataset into a testing and training set. The training set will consist of a sample of 80 % of the whole dataset, the testing set the remaining 20 %. 

```{r echo = FALSE}
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

summary(test)
```

## Linear Discriminant Analysis

Linear Discriminant Analysis (LDA) is a classification method for continuous normally distributed classes such as in our scaled dataset in this analysis. Using _crime_ as our target variable, we shall use LDA on our training set so as to later test it on the testing set

```{r echo = F}
lda.fit <- lda(crime ~., data = train)
lda.fit
```

Based on the above, the 1st linear discriminant (LD1) explains as much as 95 % of the variance, LD2 explaining 3.6 % and LD3 1.2 %. Visualising the observations across LD1 and LD2, the data looks as follows: 

```{r echo = F}
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```

### Predicting classes in test data

Based on the classification data of the LDA above, we shall classify the observations of the test data, i.e. we are testing the predictive accuracy of the LDA on data not part of the model's training data. The results of the prediction are as follows: 

```{r echo = F}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

Out of a total of 102 observations, the  number of incorrectly classified observations is 30 = 4 + 9 + 9 + 5 + 2 + 1, i.e. 29.4 %. 

## Distances

Here we shall investigate the distances between observations. To do this, we begin with a fresh dataset, i.e. the original Bostond dataset prior to the modifications described above. We standardise the dataset and 

```{r eval = F}
data('Boston')
boston_scaled <- scale(Boston)

```
```{r echo = F}
# euclidean distance matrix
dist_eu <- dist(Boston)

print("Euclidean distance")
# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled, method = "manhattan")

print("Manhattan distance")
# look at the summary of the distances"
summary(dist_man)
```

Let us visualise the k-means of the dataset. Here the optimal number of clusters is determined by looking at the total of _within cluster sum of squares_ (WCSS), namely, where the line in a line plot drops rapidly.


```{r echo = F}
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```
Based on the above, we select 2 as the optimal number of clusters. Visualised, the clusters appear as follows

```{r echo = F}
#K-means clustering based on av
km <-kmeans(Boston, centers = 2)

#pairs(Boston[6:10], col = km$cluster)
km$COR <- as.factor(km$cluster)
ggpairs(data=Boston[6:11], title="Two clusters", mapping=ggplot2::aes(colour = km$COR), lower=list(combo=wrap("facethist",binwidth=1)))
```

## Super-bonus

We shall create a 3D-plot of the data across the three linear dimensions, setting the colour to represent _crime_ quantiles.
```{r echo = F}
#Code from Moodle course page
library(plotly)
model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', color = train$crime, mode='markers')
```

Looking at the same model with color signifying the clusters:
```{r echo = F}

#Creating kmB only for train, since using km for all of Boston produces error (Error: Columns `x`, `y`, `z` must be length 1 or 506, not 404, 404, 404). I assume this is because km$cluster contains all of Boston and is too long (Boston = 506, train = 404). 
kmB <- kmeans(train[1:13], centers = 2)

#Making numeric value into factors
kmB$COR <- as.factor(kmB$cluster)

#Same code as above (from Moodle)
model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

#Same as above, but using kmB
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = kmB$COR)
```

Here the _low_ and _median low_ are merged into cluster 1, whereas _high_ and _median high_ make up the second cluster. A 3D view illustrates the heterogeneity of the clusters: cluster 1 has limited y-values (LD2), but a far range of x-values (LD1), whereas for cluster 2 the opposite is true. Both clusters have values on the z-value. However, it should be noted that LD3 had the smallest proportion of trace and also that of LD2 was quite low. In light of this, the plotting seems appropriate enough.
