# Logistic regression

_In this section I use a logistic regression model to study what variables impact alcohol consumption._
```{r echo = F, eval = F}
library(tidyr); library(ggplot2); library(dplyr); library(boot)
```

## Data overview

The data for the present study is a dataset on alcohol consumption in secondary school students in two schools in Portugal. More information (and download) is available [here](https://archive.ics.uci.edu/ml/datasets/Student+Performance). 
The variables include varibales on e.g. the student's background (family size, education and employment of parents), behaviour (e.g. amount of freetime, health status) as wekk as performance in the subjects of Maths and Portuguese. In the dataset of this study, the original data has already been modified so that variables for _alcohol consumption on weekdays_ and _alcohol consumption on weekends_  has been interpreted into a single variable *alc_use*, which is the mean of the two. Further, a binary column *high_use*has been created based on *alc_use*, where "high_use = TRUE" signifies a mean alcohol consumption greater than 2. 

A list of all variables is shown below:
```{r echo = F}
alc <- read.table("data/alc.txt")
colnames(alc)
```

Several of the variables are numeric, using a Likert-scale type 1-5 range (very low - very high), including the values for alcohol consumption. Other variables are binary yes/no data, such as whether the student is in a romantic relationship. 

## Variables impacting alcohol consumption

### Hypotheses

The purpose of this study is to explore how variables predict high/low alcohol consumption among the observations. I hypothesise that alcohol consumption is related to the following four variables:

* __Sex__: it is expected that male students have a higher consumption rate than female students. 
* __Study time__: students who study longer per week are expected to have a lower alcohol consumption (negative correlation). 
* __Going out with friends__: students who go out will have a higher alcohol consumption rate (positive correlation).
* __Absences__: students with many absences from school are expected to have higher alcohol consumption (positive correlation).

The hypotheses above reflect only my current intuitions and are not based on research literature on alcohol consumption oron previous familiarisations with the data. It should also be mentioned that the purpose of this analysis is to study the relationship, but not even a strong relationship is necessarily a sign of causality, let alone a sign of which variable is the cause and which the effect.

### Distribution of chosen variables

#### 1. Sex
The first hypothesis is that male students have higher alcohol consumption than female students. It should be noted that the number of females (198) is around the same as males (184). Below the numbers of high consumption have been given in proportions. 
```{r echo = F, warning = F}
library(ggplot2)
ggplot(alc, aes(sex, ..count..)) + 
  ylab("number of students") +
  xlab("sex") +
  scale_y_continuous(labels = scales::percent) +
  geom_bar(aes(fill = high_use), position = "fill") +
  ggtitle("Proportion of high consumption by sex")

```

The figure shows that our hypothesis is supported. As can be seen, male students have a higher percentage off alcohol consumption, supporting the hypothesis that there are more men with high consumption rates than women. 

#### 2. Study time
Our second hypothesis is that students who study more are less likely to have a high alcohol consumption rate (negative correlation). Study time is measured with the following 4-point scale, representing study hours per week: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours. In the figure below, the findings are again split across sexes. A boxplot was chosen as in spite of the values being numbers, Likert-type data is better thought of as [ordinal rather than continuous data](https://measuringu.com/mean-ordinal/), and any averages or quartiles produced from it are misguiding at best. 
```{r echo = F}
library(ggplot2)
ggplot(alc, aes(studytime, ..count..)) + 
  ylab("number of students") +
  xlab("category of study time") +
  scale_y_continuous(labels = scales::percent) +
  facet_wrap(~sex) +
  geom_bar(aes(fill = high_use), position = "fill") +
  ggtitle("Study time and consumption by sex")  
```

The graph indicates that high consumption is indeed more common among students with lower weekly totals of study time. Interestingly, male students have a break in the pattern, with students who study 5-10 hours per week having a noticeably lower number of high consumers than all other groups apart from females studying more than 10 hours. However, our overall hypothesis is still supported. 

#### 3. Going out with friends
Here it is hypothesised that students who go out are more likely to have high consumption rates. The frequency of going out with friends is here measured using a numeric 5-step Likert-scale, where 1 = very low, and 5 = very high. Again, we examine this first in boxplots. 
```{r echo = F}
ggplot(alc, aes(goout, ..count..)) + 
  ylab("number of students") +
  xlab("category of going out") +
  scale_y_continuous(labels = scales::percent) +
  facet_wrap(~sex) +
  geom_bar(aes(fill = high_use), position = "fill") +
  ggtitle("Frequency of going out by alcohol consumption and sex")
```

It appears that high consumption is more common among students who go out frequently, but this is especially pronounced among male students, whereas the female students who go out the most have a lower number of high consumers than the second-highest female group. Thus, our hypothesis is applicable mainly for male students.

#### 4. Absences
Another assumption that we have is that students with high alcohol consumption are also more likely to have more absences from school. Absences are here a numeric number representing the number of absences. We shall explore the numerical value in a boxplot, where the dotted line indicates the mean. 

```{r echo = F}
ggplot(alc, aes(x = sex, y = absences, col = high_use)) + 
  geom_boxplot() + 
  stat_summary(fun.y = mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y..),  width = .75, linetype = "dashed") +
  ylab("Absences") +
  xlab("Sex") +
  ggtitle("Student absences by alcohol consumption and sex")
```

The means indicate a difference, but this may be because of outliers. Not counting the outliers marked as dots beyond the whiskers, the boxplots above suggest that there may be a relationship between absences and alcohol among male students, but among female students the differences are quite small, the medians being about even and the main differences being visible in the top 50%. Based only on this, we cannot outright claim that this supports our hypothesis. 

### Fitting a logistic regression model

In this section we shall fit a logistic regression model that could explain and predict high alcohol use among the population. We shall at least not yet remove any of the variables, and our model and its statistical summary is thus as follows. 

> **high_use ~ sex + studytime + goout + absences** 

```{r echo = F}
m <- glm(high_use ~ sex + studytime + goout + absences, data = alc, family = "binomial")
summary(m)
```

Based on the above, our highly statistically significant p<0.001 variables are _goout_ and _absences_, with the variable _sex_ having a statistically significant p-value of <0.01, and _studytime_ having a statistically significant p-value of <0.05. In other words, we fail to reject the null hypotheses for all our variables.

The coefficients of the variables are as follows:

```{r echo = F}
coef(m)
```

As hypothesised, _study time_ is the only explanatory variable with negative correlation. _Absences_ has the lowest absolute coefficient. We shall take the exponents of the coefficients to analyse the odds ratios of the variables as well as their confidence intervals.Â¨

Odds are calculated as p/(1-p), where _P_ is probability. The odds ratio is the ratio of the odds of two values, thus quantifying their relationship. The odds ratio also represents the increase in odds for an increase in the variable by one unit. The confidence interval then explains the precision of the odds ratio. A 97.5 % confidence interval indicates a range for how precise the odds ratio is: the interval indicates a range with 97.5 % probability of this calculation being correct for this population. 

```{r echo = F, warning = F}
library(dplyr)
OddsRatio <- coef(m) %>% exp
ConfidenceInterval <- confint(m) %>% exp
cbind(OddsRatio, ConfidenceInterval)
```

The odds ratios show the increase in the odds for increased high alcohol consumption for every increase in category of _study time_, _going out_ or _absences_. For _sex_, it indicates that the odds of a man having high alcohol consumption is 2.18 higher than women. Ranked from highest to lowest odds ratio, i.e. the most significant change in odds per unit, _sex_ is at the top and _study time_ the lowest. However, the confidence intervals are fairly wide, meaning that a large range is needed to be 97.5 % certain of the matter when it comes to the total population. 

### Predictive accuracy

Finally, we shall evaluate the prediction accuracy of our logistic regression model. That is, if the model was used to predict each observation, with what frequency would it be able to correctly categorize an observation as having either high or low alcohol consumption rate. In this case the accuracy of our model is as follows (in n of observations and proportions):

```{r echo = F, warning = F}
library(dplyr)
probabilities <- predict(m, type = "response")

# create new column with predictions
alc <- mutate(alc, probability = probabilities)

# new column for whether prediction was accurate
alc <- mutate(alc, prediction = probability > 0.5)

table(high_use = alc$high_use, prediction = alc$prediction)
table(high_use = alc$high_use, prediction = alc$prediction) %>% prop.table %>% addmargins
```

The so-called confusion matrix above (with rounded proportions) indicates that our model categorises 66.0 % (N = 252) of the observations have a low consumption rate and are by the model predicted as such. Correspondingly, 12.8 % (N = 29) of observations have both high consumption and are accurately predicted as such. The remaining were predicted incorrectly (have high use but were predicted as low, or vice versa). 

In short, approx. 78.9 % of the observations were predicted correctly and 21.2 % (0.2120419) were predicted incorrectly. Visualised, the data appears as follows:

```{r echo = F}
ggplot(alc, aes(x = probability, y = high_use, col = prediction)) +
  geom_point() +
  ylab("High Use") +
  xlab("Prediction") +
  ggtitle("Prediction accuracy")
```

Supposing that we had not fitted a logistic regression model but simply guessed our way, the number of correct guesses can be different (ideally lower) than the number of corrects predicted by the model. If we assumed none of the students to have a high alcohol consumption rate (probability = 0): our error rate would be as the following:

```{r echo = F}
# function for calculating mean prediction error
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc$high_use, prob = 0)
```

We shall also test the predictive power of a 50/50 guessing method where the probability for a student to have a high alcohol consumption rate is assumed to be 0.5 (i.e. the rate is as likely to be high as to be low). We calculate the mean of incorrect guesses as:

```{r echo = F}
# function for calculating mean prediction error
loss_func2 <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func2(class = alc$high_use, prob = 0.5)

```
Interestingly, our calculation would indicate a mean of incorrect guesses is 0, i.e. that all guesses are correct. I find this somewhat suspicious, but unfortunately the investigation of the accuracy of this falls outside of the scope of this analysis.

## Bonus

### 10-fold validation

Lastly, we shall use the so-called 10-fold validation to assess the accuracy of the model fitted above. This method is a form of cross-validation and the main goal is therefore to validate the model on an independent data set that has not been used for training the model. In 10-fold validation, the data is split into 10 parts, from which 9 parts are used for training, and the remaining 1 for testing. The process is then repeated with a different part functioning as testing set. The mean prediction error is as follows:

```{r echo = F}
library(boot)
#10-fold cross-validation
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m, K = 10)
# average number of wrong predictions in the cross validation
cv$delta[1]
```

In summary, the predictive of the model on test data is very close to the predictive power of our model (0.2120419).
The predictive error is also lower than that of a similar model tested in the DataCamp exercises leading up to this analysis, where the explanatory variables were assumed to be _sex_, _absences_ and _failures in classes_. 